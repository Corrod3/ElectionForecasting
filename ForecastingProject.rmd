---
title: | 
    | Mobile Phone User Polling for the German Federal Election 2017
subtitle: "Election Forecasting Project"
author: "Moritz Hemmerlein & Alexander Sacharow"
abstract: "Using online surveys to forecast election outcomes imposes severe challenges to pollsters. Non-representativ samples and likely-voter bias skew gathered information and require adequate adjustment. This paper sets out to discuss different approaches to adjust a non-representative online polls on the upcoming german federal election and how they can be applied to a poll conducted with mobile phone app users."
header-includes:
    - \usepackage{fancyhdr}
    - \pagestyle{fancy}
    - \fancyhead[LO,LE]{Non-represenative polling}
    - \fancyfoot[LO,LE]{Election Forecasting Project}
    - \fancyfoot[RE,RO]{Moritz Hemmerlein \\ Alexander Sacharow}
    - \usepackage{graphicx}
    - \usepackage{setspace}
    - \usepackage{subfig}
    - \onehalfspacing
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    toc: true
    number_sections: true
fontfamily: mathpazo
fontsize: 11pt
urlcolor: blue
bibliography:
    -  literature.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Clear Global environment
rm(list=ls())

# Setting Working directory
try(setwd("D:/Eigene Datein/Dokumente/Uni/Hertie/Materials/Election Forecasting/ElectionForecasting"), silent = TRUE)
try(setwd("C:\\Users\\Moritz\\Desktop\\ElectionForecasting"), silent = TRUE)

source("main.R")

# Collect packages/libraries we need for paper:
packages <- c("fontcm", "psych", "plotly")

# install packages if not installed before
for (p in packages) {
  if (p %in% installed.packages()[,1]) {
    require(p, character.only=T)
  }
  else {
    install.packages(p, repos="http://cran.rstudio.com", dependencies = TRUE)
    require(p, character.only=T)
  }
}
rm(p, packages)
```
\newpage

# Introduction

<!--The problem -->
The digitialization is challenging the way polling was done for many decades. Calling people on their landline phones has become difficult as response rates dropped and households equiped with landline phones are getting less and less [@Skibba.2016]. In response, polling institutions have resorted to other methods for polling ranging from face-to-face interviews to mobile phone calling. However, these methods either face similar difficulties to ensure representativeness or are too expensive for regular polling. In order to tackle these obstacles, pollsters are increasingly using online polls, which are cheap and fast, but can be highly non-representative [e.g. @Wang.2015].

<!-- The problem [cont.] -->
The eventual aim of pollsters in online as well as traditional polling is to collect sample data that reflects the view of a population of interest. The major difference between both methods is that with most online polls representativeness cannot be ensured before the actual poll takes place. In online surveys respondents are more likely to be from certain age groups or with a particular political background depending on the website or app where the survey is conducted. Such a non-representative poll can, however, be statistically adjusted to match the demographic composition of the population.

Additionally, online election polls, like traditional ones, face a another problem. Election forecasters are naturally not only interested in the population as such, but in the population of actual voters. By the time a poll is made representative in demographic terms, it is still in question whether it reflects the group of people who cast their ballots. This is, however, crucial in order to make an accurate prediction. Traditional polling tries to account for this using likely voter models and could perform fairly well [@Gallup.2010, @Keeter.2016]. Online surveys will also have to be adjusted to actual voting population in order to provide accurate predictions.

<!-- aim / question -->
In the paper at hand we want to explore and analyse how different approaches to adjust online polls perform. Our first approach will be a two step procedure where the polling data is first made representative of the population and then likely voter methods are used to resemble the probable voters population. Our second approach attempts to combine both steps into one by adjusting the online polls directly to exit polls and voting statistics from previous elections. For this we work with individual level data from mobile-phone app users who were surveyed on their vote intention in the German federal election 2017. As the ultimate election will only be after the end of research, we use other forecasts of the 2017 federal election as a benchmark.

<!-- LATER: here results of our paper -->

<!-- Structure  -->
The structure of the paper is as follows: Section two will survey the literature on non-representative polls and approaches to employ such data to forecast elections. Subsequently, we present our data and discuss possible problems with it. Afterwards we discuss the methodology we want to use for adjusting our data at hand. 
<!-- To be included later: Chapter five is presenting and dicussing our preliminary results. Chapter six concludes and gives an outlook on practical obstacles and methodological issues that our approach suffers from. -->

# Related Literature

<!-- problems of traditional polling -->
Traditional polling and in particular election polling has relied heavily on telephone surveys for the last decades. To ensure representativeness the standard was randomized digit dialing (RDD). The selection of random respondents was intended to eliminate the sample bias of the survey. However, for several reasons this approach has become unreliable. First, response rates have declined heavily [@Keeter.2006; @Holbrook.2007]. @PewResearchCenter.2012  reported that in the U.S. the response rate was down to 9% in 2012, compared to 36% in 1997. This is mainly a result of technical changes like the possible identification of caller. As a result polls have a tremendious non-response bias which cannot be handled with classical approaches anymore. Second, more and more people don't get landlines telephones after moving to new places or just give them up as mobile phone and other means of communication have increasingly become popular. This induces a sample bias which the RDD approach intended to eliminate. Hence, classical representive  polling is becoming less reliable and this trend will rather continue than cease. Unsurprisingly, lacking representativeness of surveys as been identified as a core reason for recent election polling failures, e.g. in the UK General elections 2015 [@Mellon.2015].

<!-- Papers on non-representative polling -->
Can non-representative polls like online polls fix this? Since the famously failure of Literature Digest poll in 1936 U.S. presidential election, pollsters have been sceptical of non-representative polling [@Squire.1988, @Goel.2017]. This scepticism is still shared among many pollsters. @Yeager.2011, for example, argue that phone surveys are still more accurate then online polls. However, they only use simple correction approaches for the non-probability sampled online polls.^[@Goel.2017 [Footnote 2] where able to get more accurate results using the basic approach, but in a different way.]

@Wang.2015 in contrast are much more optimistic about the possiblities of non-representative polling. They used polling results from XBox users which were highly unrepresentative of the population to forecast the 2012 U.S. presidential elections. By employing a sophisticated multi-stage approach to stratify and calibrate the data they were able to generate accurate forecasts of the elections. However, their methodology is mainly suited for large data sets which have sufficient observations for the sub-group in the sample. @Goel.2017 showed that smaller non-representative online surveys can also be accurate. They  conducted polls on [Amazon Mechanical Turk](https://www.mturk.com/mturk/welcome) and a mobile phone app and achieved a level of accuracy sufficient for most applications. These works shows that online survey can be used in a meaningful way, if appropiate methods are used to stratify and calibrate the non-representative data.


<!--  papers on likely voter estimation (if we include it) 
But even if the problems of non-representative

Likely voter modelling is notoriously the secret-sauce aspect of polling, says Kennedy Courtney Kennedy, director of survey research at the Pew Research Center in Washington DC (from @Skibba.2016)

Typical biases in polling (most important: non-response bias; sample-selection bias)
    Which biases do we expect in the data -->
    
   

# Data and Potential Biases

## Europulse Survey

In this paper, we are using data from [Dalia Research](https://daliaresearch.com/), an online polling firm which is conducting market and opinion research exclusively through smartphones. To ensure to collect data from a broad variety of target populations, Dalia is using a diverse set of app and website categories such as sports, news, entertainment or games. To control how serious participants answer the survey an algorithms analyses the consistency and the response behaviour and computes a "trust score" to every respondent. Dalia praises its methodology as distinctively accounting for potential biases such as interviewer effect, social desirability bias or interviewer data entry errors. [@Dalia.2016] 

Our forecasting project utilizes data of Dalia Research's Europulse Survey which is conducted quarterly in all EU countries. The survey consists of seven waves, but for this project we only use two waves of the survey from December 2016 and March 2017. The first wave is freely available on [Kaggle](https://www.kaggle.com/daliaresearch/trump-effect), the second wave was provided to us directly by Dalia Research. Each wave consists of about 11000 individuals, of which roughly 1900 were from Germany which is the fraction of respondents we will focus on in the following analysis. The data is already pre-stratified by Dalia Research based on micro zensus data for age and gender. 

The Europulse data is not particularly collected for election forecasting purposes but contains data on a variety of questions such as online behaviour, media consumption and personal views on political and societal development in the European Union and the country of origin. Moreover, the survey contains information on the respondents personal background, demographic data and his or her financial situation. These data can be utilized in order to improve the representativeness of the survey through weighting. This will be explained more detailed in the methodology section.

The election-related variables collected in the Europulse survey are similar to traditional vote intention polling questions. First of all, the respondents are asked if and for which party they will vote in the upcoming election and for which party they voted in the previous election. Moreover, they are asked to rank political parties and describe the degree of certainty to cast a ballot for a particular party.

## Potential Sources of Bias

As with all surveys, the methodology of Dalia Research has several sources of potential biases. In the case of the Europulse survey they are primarily from flawed measurement and representation of the surveyed population, as listed in the figure below [@Groves.2009].

\begin{figure}[ht]
  \centering
  \includegraphics[width=5in]{./Grafiken/SourcesOfSurveyError}
  \caption{Potential sources of survey error} 
\end{figure}

<!-- measurement error -->
With regard to measuring vote intention to vote for a particular party, the Europulse survey uses a similar approach as traditional surveys: it asks respondents directly which party they intent to vote for at the upcoming election. Whether such questions measure correctly the actual voting behaviour at the election day is questionable but the approach does not differ from other polling methods in its validity<!-- source needed-->. However, online surveys, such as Europulse, can reasonably claim to avoid some sources of measurement error such as social desirability bias or interviewer bias. Since such surveys are often anonymous, the social pressure on the respondent is neglectable, and interaction with the interviewer does not bias the response.

<!-- representativeness: Self-selection bias and sampling error --> 
Regarding representativeness, the Europulse survey has some limitations in comparision to face-to-face or RDD interviews. First of all, Europulse' framework does not select participants at random but offers visitors of certain websites or app-users the opportunity to participate in the survey. Hence, this approach carries the risk of self-selection. Moreover, representativeness would require that the Europulse survey in principle should be available to the entire population. However, not everyone is using smart phones and even if they are, even less individuals used the applications Dalia uses for its surveys. The users Dalia Research actually tends to reach are likely to be much younger and technology oriented than the general population would be. This is in particular a problem for election polling, as older voters are systematically underrepresentated in such online methodologies.

\begin{figure}[!tbp]
  \centering
  \subfloat[Internet usage by   age]{\includegraphics[width=0.4\textwidth]{./Grafiken/InternetnutzerAlter.png}}
\hspace{0.05\textwidth}
  \subfloat[Voter turnout by age]{\includegraphics[width=0.4\textwidth]{./Grafiken/WahlbeteiligungAlter.png}}
\end{figure}

<!-- In addition, through self-selection the sample will be most likely be biased towards the sub-sample of the plattform users that is more prone to surveys in general, e.g. people with higher education or scientific interest. [Source?] 

A: is basically already mentioned above -->

<!-- Dalia response -->
Dalia Research tries to account for these problems. First, they try to increase the diversity of their respondents by presenting their surveys on various apps and plattforms targeting different user groups. Second, they pre- and post-stratify their data. Pre-stratification is done on the basis of age and gender, using elf-reported demographic information. For each age and gender strata they target a certain number of respondents so that their sample ressembles roughly the German population. In a second step they use data from the German Census an compute weights for combined cluster of age, gender, education and whether the respondent lives in an urban or rural area. These weights can finally be used to post-stratify the sample to match the demographic composition of the population more closely. Third, the selection bias is sightly decreased by the fact that respondents are not informed that they are answering an election related survey before they actually start it and in general Dalia Research has high completion rates, hence participants are only rarely dropping out after they started a survey.

<!-- The Dalia Research methodology tries to ensure this by using various online apps and plattforms to draw their respondents from different stratas of the population. --> 

Despite these efforts it is questionable whether the data can be called representative. First of all, even if the sample represents the German population in terms of age groups and gender, it can be question whether the old female or male users are truely representative of their age group. And not revealing the survey content might sort out political motivated respondents, but interest in political matters is likely correlated to general willingness to respond to surveys. <!-- Most important, since it is not conducted particularly as an election poll, it is likely to come with several other biases such as likely voter bias or late swing. [@Mellon.2015] A: this is not logical--> Hence, in order to utilize the data to election forecasting further weighting and post-stratification will be conducted. Different methods to do so and their technical details will be explained in the following.
 
## Data for Post-stratification and Weighting

The data used for post-stratification and weighting come from several sources. To adjust our sample to demographic composition of the German population we use data of the German [Zensus 2011](https://www.zensus2011.de/DE/Home/home_node.html). The data is freely available and we obtained combined frequencies for characteristics such as age, gender, education, employment status and confession. The Census claims to reflect the actual demgraphic distibution of the German population and hence is suitable in order to post-stratify our sample to demographic criteria.

Furthermore, we collected data from election polls conducted by Forschungsgruppe Wahlen e.V., Infratest dimap and from the official German election statistics [@KAS.2013]. While the latter contains only information on turnout and the demographic dimensions age and gender, Forschungsgruppe Wahlen e.V. has also issued election results and turnout along groups with different education, different employment status and confession. Such data the closes estimate of the actual voting population and their voting behaviour across demographic groups and across the spectrum of political parties. We will employ such data to account for likely voter bias in the Europulse data at hand.

<!-- more data could be obtained form ALLBUS social survey? -->

DIESE GRAFIKEN NOCH TEXTLICH EINBAUEN

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\textwidth]{./Grafiken/VoteNextElection}
  \caption{Federal election 2017 vote intent (Unweighted Europulse)}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\textwidth]{./Grafiken/TurnoutByGender}
  \caption{Self-reported turnout (Unweighted Europulse)}
\end{figure}

<!-- noch kurs: Als vergleichs daten nehmen wir daten von wahlrecht.de-->

# Methodology

We want to employ two approaches generate election forecasts from the data we have. In the first approach the survey data is adjusted in a two step procedure: First, the representativeness of survey data is increased by finding appropiate weights. Then a likely voting model is used to get to the election forecast. Our second approach does this in one step: The survey data will be directly adjusted to the likely voter population based on exit poll data from the last German federal election. 

In order to follow both procedures, we need methods to compute weights for sub-groups of the survey sample aiming at increased representativeness. A classical way to get these weights is *raking*. With raking weights are assigned to each respondent in order to match the marginal distribution of characteristics in the 'true' population. For example, if we know the distribution of education level and employment status in the population, for each we can compute a weight so that the weighted survey sample has the same distribution of these characteristics as the 'true' population. Basically, raking computes a joint distributions for each combination of characteristics. 

In formal terms, we can describe each combination of an individual and a characteristic with $x_{i,j} \in \{0,1\}$ where $i$ stands for the individual and $j$ for the characteristic. To illustrate, if $x_{i,j} = 1$ an individual may be female and if $x_{i,j} = 0$ an individual is not female. Hence, each characteristic is modeled with a binary variable. $c_j$ expresses the prevailance of a characteristic in the population and raking estimates the weights $w_i$ such that:

\begin{equation}
c_j = \frac{\sum_{i = 1}^{n}w_i x_{i,j}}{\sum_{i = 1}^{n}w_i} \forall j
\end{equation}

The weights can than be used to compute the raking estimates for each observation in the survey $y$:

\begin{equation}
\hat y^{rake} = \frac{\sum_{i = 1}^{n}w_i y_{i}}{\sum_{i = 1}^{n}w_i} 
\end{equation}

<!-- @Goel.2017 use the anesrake package to execute this -->

However, raking is not ideal. It is often used if only marginal probabilities are available. This is also the case for some of our data. However, if the joint distribution of characteristics is known we can use *post-stratification* instead. In contrast to raking, post stratification takes into account the distribution of combination of characteristics in the population. In other words: It sub-divides the population into stratas for each combination Unfortunately, publically available data often only includes the distribution of pairs of characteristics or in rare cases triples (e.g. age, gender and past vote). Ideally, we would like to know the distribution of the population for each strata (combination of the characteristics).

Formally the post-stratification estimator can be expressed as follows [@Goel.2017]:

\begin{equation}
  y^{post} = \frac{\sum^{J}_{j=1}N_J \hat y_j}{\sum^{J}_{j=1}N_J}
\end{equation}

where $\hat y^{post}$ is the estimator of y in the strata j and $N_J$ the size of the j-th strata in the population. But even if the size of $N_J$ is known for each strata, the number of strata's grows exponentially with each characteristic included. If we use for instance two gender categories and four age groups we have eight stratas, if we add past vote (7 categories) we have immediately 54. As a result we might only have a few individuals for each strata or even none. Hence, a few respondents in the strata female, old (60+) and FDP voter in 2013 will have an overproproportional influence on the strata estimate. 

If possible, we might use *model-based post-stratification* to counter this effect. In this approach the estimates for each strata are not based on the average in the strata, but the result of a multinomial logistic regression. In order to arrive at this regression results, demographic variables in the sample can be used. In order to execute this we will orient ourself at the work of @Goel.2017.^[They are developing an r package for this (postr) and might be willing to share their r script (as they already anounced to make ot public).] The larger question here is, whether it makes any sense at all to use model-based post-stratification if we only have the strata size for triples at best.

<!-- too detailed for the introduction 
The general solution to non-representativeness is weighting. With online surveys you need to do a little more, instead of doing a stratification-design to draw from certain strata you need to define the strata afterwards and weight your responses accordingly (post-stratification). This can be done with Census data that gives you the frequencies of the strata in the population. Then, when you made your survey representative of the population, in a second step you do what all polls have to do. Account for self-selection and non-response bias and actual-voter bias: -> Therefore, weight according to likelihood of actual voting and party affiliation of certain strata. (Here I want to point out the two steps!)

Now: Our actual project. In our forecasting project we want to use non-representative polling data from mobile-phone app users. (Obtained from Dalia research, citation blabla) So we have a raw survey and we want to weight it. We want to dicuss severall weighting methods. And we want to make a forecast of the Bundestagswahl. blalablablubb -->

<!-- from Lumpley slides
Nonresponse introduces statistical inefficiency
•Nonresponse will also lead to biased estimates if the propensity to respond to the survey(or an item) is related to the attribute of interest
•Ex ante attempts to reduce unit and item nonreponse seldom are fully successful (e.g. increase and dispersion of contact attempts, incentives, conversion by specially trained interviewers, reduction of respondent burden/ interview length via matrix sampling etc.)
•Non response rate and non response bias seem hardly related
•Importance ofex post strategies for missing data
•Different strategies are based on different assumptions about missingness and different kinds of information available on those missing

Poststratification
•Ex poststratification of the sample along one or several categorical variables x…
-… which are suspected to co-determine missingness and the attribute of interest y („commoncauses“ according to Groves, 2006)
-… whose (joint) population distribution is known
•Conceive of the categories/ cross-classifications of x as groups or strata h, use the stratified estimator
•Corrects for disproportionalities between NhN and nh′N that may be either due to sampling or due to non response
•Assumes MAR

Raking
-> if only marginal distributions are known (as for example with the forschungsgruppe wahlen)
--> 

## Approach number one

For stratifying the data we orient ourself at the work of XXX. The basic idea is to compute clusters of voters along several demographic categories and use their past votes to compute weights.

First, if possible we use post-stratification (see Lumpley, ch. 7?) to compute weights for subgroups in the sample. Post-stratification tries to make a sample representative of the actual population by ensuring the relative size of subgroup resembles the relative size of the same subgroup in the 'true' population. For forecasting the 'true' population is not known, as it is a question of who will actually turn out to vote. 

How we plan to make poll representative

Compare different stratification approaches

Likely problems we will encounter: 1. empthy clusters or clusters with low number of observations. Implications: If empthy, there is a real problem. If the number of observation is low, e.g. below 20, the weigths will will amplify the impact of this small group in the total forecasting result.

Benchmark -> other publically available polls. This is straight-forward, but also problematic as it might induce a herding effect. The final evaluation is only possibe after the election

## Approach number two


# Data Overview

how representative our data already is

1. raw data forecast. Compared to other forecastes the data under represents the CDU as well as the SPD. (Verify) 

2. Show distribution of respondents on different demographic clusters and compare to zensus / exit polls / election statistics

3. Raw (voted last election)

# Results

What the result is of making it representative

1. Election forecast Weighted with exit polls

2. Election forecast weighted with election statics 

3. Election forecast weighted with zensus

Compare the three different weighten approaches

Comment Moritz: I think we have to weight the data with Zensus data in any case at least for gender and age as long we don't want to use the Dalia weights; then we can either use election statistics or exit poll data (that we don't have)

My approach would be:

1. Weighting with Zensus (accounts for self-selection of the survey)
2. Different mixes of weighting with election statistics with education
(accounts for likely voter bias)


# Conclusion

Summary of the core finding

Further implications

# References

<!-- to be copied in the .bib file
% This file was created with Citavi 5.5.0.1

@misc{PewResearchCenter.2012,
 date = {2012},
 title = {Assessing the Representativeness of Public Opinion Surveys},
 url = {http://www.people-press.org/files/legacy-pdf/Assessing%20the%20Representativeness%20of%20Public%20Opinion%20Surveys.pdf},
 institution = {{Pew Research Center}}
}





@article{Skibba.2016,
 author = {Skibba, Ramin},
 year = {2016},
 title = {The polling crisis: How to tell what people really think},
 keywords = {Bias (Epidemiology);Canada;Cell Phones/economics/utilization;Continental Population Groups/statistics {\&} numerical data;Data Collection/methods/standards;Datasets as Topic;Demography/statistics {\&} numerical data;Educational Status;Female;Humans;Internet/economics/utilization;Male;Politics;Public Opinion;Surveys and Questionnaires/economics/standards;Uncertainty;United Kingdom;United States},
 urldate = {01.04.2017},
 pages = {304--306},
 volume = {538},
 number = {7625},
 journal = {Nature},
 doi = {10.1038/538304a}
}

@article{Wang.2015,
 abstract = {International Journal of Forecasting, Corrected proof. doi:10.1016/j.ijforecast.2014.06.001},
 author = {Wang, Wei and Rothschild, David and Goel, Sharad and Gelman, Andrew},
 year = {2015},
 title = {Forecasting elections with non-representative polls},
 keywords = {Election forecasting;Multilevel regression and poststratification;Non-representative polling},
 urldate = {01.04.2017},
 pages = {980--991},
 volume = {31},
 number = {3},
 issn = {01692070},
 journal = {International Journal of Forecasting},
 doi = {10.1016/j.ijforecast.2014.06.001}
}

@article{Mellon.2015,
 author = {Mellon, Jonathan and Prosser, Chris},
 year = {2015},
 title = {Investigating the Great British Polling Miss: Evidence from the British Election Study},
 issn = {1556-5068},
 journal = {SSRN Electronic Journal},
 doi = {10.2139/ssrn.2631165}
}

@article{Keeter.2006,
 abstract = {Declining contact and cooperation rates in random digit dial (RDD) national telephone surveys raise serious concerns about the validity of estimates drawn from such research. While research in the 1990s indicated that nonresponse bias was relatively small, response rates have continued to fall since then. The current study replicates a 1997 methodological experiment that compared results from a {\dq}Standard{\dq} 5-day survey employing the Pew Research Center's usual methodology with results from a {\dq}Rigorous{\dq} survey conducted over a much longer field period and achieving a significantly higher response rate. As with the 1997 study, there is little to suggest that unit nonresponse within the range of response rates obtained seriously threatens the quality of survey estimates. In 77 out of 84 comparable items, the two surveys yielded results that were statistically indistinguishable. While the {\dq}Rigorous{\dq} study respondents tended to be somewhat less politically engaged, they did not report consistently different behaviors or attitudes on other kinds of questions. With respect to sample composition, the Standard survey was closely aligned with estimates from the U.S. Census and other large government surveys on most variables. We extend our analysis of nonresponse to include comparisons with the hardest-to-reach respondents and with respondents who terminated the interview prior to completion.},
 author = {Keeter, Scott and Kennedy, Courtney and Dimock, Michael and Best, Jonathan and Craighill, Peyton},
 year = {2006},
 title = {Gauging the Impact of Growing Nonresponse on Estimates from a National RDD Telephone Survey},
 url = {http://www.jstor.org/stable/4124225},
 pages = {759--779},
 volume = {70},
 number = {5},
 issn = {15375331},
 journal = {The Public Opinion Quarterly}
}

% This file was created with Citavi 5.5.0.1

@article{Yeager.2011,
 author = {Yeager, D. S. and Krosnick, J. A. and Chang, L. and Javitz, H. S. and Levendusky, M. S. and Simpser, A. and Wang, R.},
 year = {2011},
 title = {Comparing the Accuracy of RDD Telephone Surveys and Internet Surveys Conducted with Probability and Non-Probability Samples},
 pages = {709--747},
 volume = {75},
 number = {4},
 journal = {Public Opinion Quarterly},
 doi = {10.1093/poq/nfr020}
}

@incollection{Holbrook.2007,
 author = {Holbrook, Allyson L. and Krosnick, Jon A. and Pfent, Alison},
 title = {The Causes and Consequences of Response Rates in Surveys by the News Media and Government Contractor Survey Research Firms},
 pages = {499--528},
 publisher = {{John Wiley {\&} Sons, Inc}},
 isbn = {9780470173404},
 editor = {Lepkowski, James M. and Tucker, Clyde and Brick, J. Michael and Leeuw, Edith D. de and Japec, Lilli and Lavrakas, Paul J. and Link, Michael W. and Sangster, Roberta L.},
 booktitle = {Advances in Telephone Survey Methodology},
 year = {2007},
 address = {Hoboken, NJ, USA},
 doi = {10.1002/9780470173404.ch23}
}


@article{Squire.1988,
 author = {Squire, Peverill},
 year = {1988},
 title = {Why the 1936 Literary Digest Poll Failed},
 url = {http://www.jstor.org/stable/2749114},
 urldate = {01.04.2017},
 pages = {125--133},
 volume = {52},
 number = {1},
 issn = {15375331},
 journal = {The Public Opinion Quarterly}
}

@book{Groves.2009,
author = "Groves, Robert M. and Floyd J. Fowler Jr. and Mick P. Couper and James M.
Lepkowski and Eleanor Singer and Roger Tourangeau",
title = "Survey Methodology",
publisher  = "John Wiley & Sons Inc",
year = "2009",
address = "New Jersey",
edition = 2
}



-->






