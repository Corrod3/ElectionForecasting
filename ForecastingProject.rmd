---
title: |
  | Non-Representative Polling
  | Mobile User Polling Data for the German Federal Election 2017
subtitle: "Election Forecasting Project"
author: "Moritz Hemmerlein & Alexander Sacharow"
abstract: "Using online surveys to forecast election outcomes imposes severe challenges to pollsters. Non-representativ samples and likely-voter bias skew gathered information and require adequate adjustment. This paper sets out to discuss different approaches to adjust a non-representative online polls on the upcoming german federal election and how they can be applied to a poll conducted with mobile phone app users."
header-includes:
    - \usepackage{fancyhdr}
    - \pagestyle{fancy}
    - \fancyhead[LO,LE]{Non-represenative polling}
    - \fancyfoot[LO,LE]{Election Forecasting Project}
    - \fancyfoot[RE,RO]{Moritz Hemmerlein \\ Alexander Sacharow}
    - \usepackage{graphicx}
    - \usepackage{setspace}
    - \usepackage{subfig}
    - \onehalfspacing
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    toc: true
    number_sections: true
fontfamily: mathpazo
fontsize: 11pt
urlcolor: blue
bibliography:
    -  literature.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Clear Global environment
rm(list=ls())

# Setting Working directory
try(setwd("D:/Eigene Datein/Dokumente/Uni/Hertie/Materials/Election Forecasting/ElectionForecasting"), silent = TRUE)
try(setwd("C:\\Users\\Moritz\\Desktop\\ElectionForecasting"), silent = TRUE)

source("main.R")

# Collect packages/libraries we need for paper:
packages <- c("fontcm", "psych", "plotly")

# install packages if not installed before
for (p in packages) {
  if (p %in% installed.packages()[,1]) {
    require(p, character.only=T)
  }
  else {
    install.packages(p, repos="http://cran.rstudio.com", dependencies = TRUE)
    require(p, character.only=T)
  }
}
rm(p, packages)
```
\newpage

# Introduction

<!--The problem -->
The digitialization is challenging the way polling was done for many decades. Calling people on their landline phones has become difficult as response rates dropped and households equiped with landline phones are getting less and less [@Skibba.2016]. In response, polling institutions have resorted to other methods for polling ranging from face-to-face interviews to mobile phone calling. However, these methods either face similar difficulties to ensure representativeness or are too expensive for regular polling. In order to tackle these obstacles, pollsters are increasingly using online polls, including highly non-representative polls [@Wang.2015].

<!-- The problem [cont.] -->
The eventual aim of pollsters in online as well as traditional polling is to collect sample data that reflects the view of a population of interest. The major difference between both methods is that with most online polls representativeness cannot be ensured before the actual poll takes place. In online surveys respondents are more likely to be from certain age groups or with a particular political background depending on the website where the survey is conducted. Such a non-representative poll can, however, be statistically adjusted (post-stratification) to match the demographic composition of the population.

Additionally, online election polls, like traditional ones, face a another problem. Pollsters are naturally not only interested in the population as such, but in the population of actual voters. By the time a poll is made representative in demographic terms it is still in question whether it reflects the group of people who cast their ballots. This is, however, crucial in order to make an accurate prediction. Traditional polling tries to account for this using likely voter models and could perform fairly well [@Gallup.2010, @Keeter.2016]. Online surveys will also have to be adjusted to actual voting population in order to provide accurate predictions.

<!-- aim / question -->
In the paper at hand we want to explore and analyse how different approaches to adjust online polls perform. Our first approach will be a two step procedure where the polling data is first made representative of the population and then likely voter methods are used to resemble the probable voters population. Our second approach attempts to combine both steps into one by post-stratifying the online poll with exit polls and voting statistics from previous elections. For this we work with individual level data from mobile-phone app users who were surveyed on their vote intention in the German federal election 2017. As the ultimate election will only be after the end of research, we use other forecasts of the 2017 federal election as a benchmark.

<!-- LATER: here results of our paper -->

<!-- Structure  -->
The structure of the paper is as follows: Chapter two will survey the literature on non-representative polls and approaches to employ such data to forecast elections. Subsequently, we present our data and discuss possible problems with this data set. Afterwards we present the methodology we want to use for adjusting our data at hand. 
<!-- To be included later: Chapter five is presenting and dicussing our preliminary results. Chapter six concludes and gives an outlook on practical obstacles and methodological issues that our approach suffers from. -->

# Related Literature

<!-- problems of traditional polling -->
Traditional polling and in particular election polling has relied heavily on telephone surveys for the last decades. To ensure representativeness the standard was randomized digit dialing (RDD). The selection of random respondents was intended to eliminate the sample bias of the survey. However, for several reasons this approach has become unreliable. First, response rates have declined heavily [@Keeter.2006; @Holbrook.2007]. @PewResearchCenter.2012  reported that in the U.S. the response rate was down to 9% in 2012, compared to 36% in 1997. This is mainly a result of technical changes like the possible identification of caller. As a result polls have a tremendious non-response bias which cannot be handled with classical approaches anymore. Second, more and more people don't get landlines telephones after moving to new places or just give them up as mobile phone and other means of communication have increasingly become popular. This induces a sample bias which the RDD approach intended to eliminate. Hence, classical representive  polling is becoming less reliable and this trend will rather continue than cease. Unsurprisingly, missing the representativeness of a survey as been identified as a core reason for recent polling failures, e.g. in the UK General elections 2015 [@Mellon.2015].

<!-- Papers on non-representative polling -->
Can non-representative polls like online polls fix this? Since the famously failure of Literature Digest poll in 1936 U.S. presidential election, pollsters have been sceptical of non-representative polling [@Squire.1988, @Goel.2017]. This scepticism is still shared among many pollsters. @Yeager.2011, for example, argue that phone surveys are still more accurate then online polls. However, they only employ simple correction approaches.

@Wang.2015 in contrast are much more optimist about the possiblities of non-representative polling. They used polling results from XBox users which were highly unrepresentative of the population to forecast the 2012 U.S. presidential elections. By employing a sophisticated multi-stage approach to stratify and calibrate the data they were able to generate accurate forecasts of the elections. However, they their methodology relied on a huge data set. But  @Goel.2017 argued that smaller non-representative online surveys can also be accurate. They  conducted polls on [Amazon Mechanical Turk](https://www.mturk.com/mturk/welcome) and a mobile phone app and achieved a level of accuracy sufficient for most applications. Their work shows that online survey can be used in a meaningful way, if appropiate methods are used to stratify and calibrate the non-representative data.


<!-- Methods on correcting polls
A: or we move this to the methological section 

1. post stratification 
2. raking
3. multiple level regression + post stratification
4. random sampling?

 papers on likely voter estimation (if we include it) 

But even if the problems of non-representative

Likely voter modelling is notoriously the secret-sauce aspect of polling, says Kennedy Courtney Kennedy, director of survey research at the Pew Research Center in Washington DC (from @Skibba.2016)

Typical biases in polling (most important: non-response bias; sample-selection bias)
    Which biases do we expect in the data -->
    
   

# Data and Potential Biases

## Europulse Survey

In this paper we are using data from [Dalia Research](https://daliaresearch.com/), an online polling firm who is conducting market and opinion research through smartphones exclusively. To ensure to collect data from a broad variety of target populations Dalia is using a diverse set of app and website categories such as sports, news, entertainment or games. To control how serious participants answer the survey an algorithms analyses the consistency and the response behaviour and computes a "trust score" to every respondent. Dalia praises its methodology as distinctively accounting for potential biases such as interviewer effect, social desirability bias or interviewer data entry errors. [@Dalia.2016] 

Our forecasting project utilizes data of Dalia Research's Europulse Survey which is conducted quarterly in all EU countries. The survey consists of seven waves, but for this project we only use two waves of the survey from December 2016 and March 2017. The first wave is freely available on [Kaggle](https://www.kaggle.com/daliaresearch/trump-effect), the second wave was provided to us directly by Dalia Research. Each wave consists of about 11000 individuals, of which roughly 1900 were from Germany which is the fraction of respondents we will focus on in the following analysis. The data is already pre-stratified by Dalia Research based on micro zensus data for age and gender. 

The Europulse data is not particularly collected for election forecasting purposes but contains data on a variety of questions such as online behaviour, media consumption and personal views on political and societal development in the European Union and the country of origin. Moreover, the survey contains information on the respondents personal background, demographic data and his or her financial situation. These data can be utilized in order to improve the representativeness of the survey through weighting. This will be explained more detailed in Chapter four.

The variables collected in Europulse to use the data for election forecasting are similar to traditional polling questions. First of all respondents are asked if and for which party they will vote in the upcoming election and for which party they voted in the previous election. Moreover, they are asked on the level of agreement with several political parties and the degree of certainty to cast a ballot, overall and for a particular party.

## Potential Sources of Bias

As with all surveys there are several sources of potential bias included in the methodology used by Dalia Research. Several of such issues that come from flawed measurement or representation of the surveyed population are listed in the figure below [@Groves.2009].

\begin{figure}[ht]
  \centering
  \includegraphics[width=5in]{./Grafiken/SourcesOfSurveyError}
  \caption{Potential sources of survey error} 
\end{figure}

With regard to measuring voters intention to vote for a particular party, Europulse uses a similar approach as traditional surveys. As such it asks directly which party a respondent intents to vote for at the upcoming election. Whether such questions measure correctly the actual voting behaviour at the election day is questionable but the approach does not differ from other polling methods to ensure validity. Moreover, online surveys such as Europulse can reasonably claim to avoid other sources of measurement error such as social desirability bias or interviewer bias. Since such surveys are often anonymous social pressure on the respondent is smaller and interaction with the interviewer can not bias the respond.

Regarding representativeness the Europulse survey is exposed to issues that face-to-face or RDD interviews do not have. First of all, Europulse' framework does not select participants completely random but offers visitors of certain websites or app-users the opportunity to participate the survey. This approach carries various risk to representativeness. With regards to population-coverage the entire group of users across all survey plattforms should be representative to the German population. However, this ist likely not to be the case for Europulse. Since online users in general tend to be younger than the German average while the voting population is older than average the Europulse sample is likely to be strongly biased with regards to the voting population. 

\begin{figure}[!tbp]
  \centering
  \subfloat[Internet usage by   age]{\includegraphics[width=0.4\textwidth]{./Grafiken/InternetnutzerAlter.png}}
\hspace{0.05\textwidth}
  \subfloat[Voter turnout by age]{\includegraphics[width=0.4\textwidth]{./Grafiken/WahlbeteiligungAlter.png}}
\end{figure}

In addition, through self-selection the sample will be most likely be biased towards the sub-sample of the plattform users that is more prone to surveys in general, e.g. people with higher education or scientific interest. []



Dalia Research tries to account for such problems using a two-step approach. Using the self-reported demographic information they stratify their sample for different age groups and gender in order to render it representative for the German population. In a second step they use data from the German Census an compute weights for combined cluster of age, gender, education and whether the respondent lives in an urban or rural area. These weights can finally be used to post-stratify the sample to match the demographic composition of the population.

Despite these efforts it is questionable if the data can match scientific standards of representativeness. Most important, since it is not conducted particularly as an election poll, it is likely to come with several other biases such as likely voter bias or late swing. [@Mellon.2015] Hence, in order to utilize the data to election forecasting further weighting and post-stratification will be conducted. Different methods to do so and their technical details will be explained in the following.
 
## Data for Post-stratification and Weighting

<!-- data for post-stratification -->
In order to post-stratify the data, we want to use different sources and compare their effect on the forecast: (1) exit poll data from Forschungsgruppe Wahlen or intratest dimap Institute (what), (2) representative election statistics and (3) microzensus data. 

<!-- data for likely-voter debate -->

General: include how did we get the data...

(dicuss biases along the framework of potential sources of bias measurement and representativeness)



# Methodology

<<<<<<< HEAD
<!-- Thought: Non-representative sampling is possible but requires large samle sizes. E.g. X-Box paper. Out sample has 1500 observations: therefore methods such as post-stratification with many post-stratifying variables not possible -->

=======
We want to employ two approaches to correct for the non-representatives of our election forecasting data. In the first approach the survey data is first corrected to be representative of the population and then the likelihood of voting is considered to arrive at an election forecast. The second approach the survey data is directly adjusted to the likely voter population based on exit poll data from the last German federal election. Both procedures we need methods to assign weights to sub-groups in the survey sample.

A classical way to correct non-representativeness is *raking*. With raking weights are assigned to each respondent in order to match the marginal distribution of characteristics in the 'true' population. For example, if we know the distribution of education level and employment status in the population, for each we can compute a weight so that the weighted survey sample has the same distribution of these characteristics as the 'true' population. Raking assumes that the joint distributions of the characteristics are not known. They are computed iterat

attempts to weight a survey to match the marginal distributions of characteristics in the population. 
assumes the marginal distribution of characteristics is known for the population. This is often the case with agehe age distribution and gender distribution in the population, but not the distribution of the combination of both features,  

This methods assigns weights to each respondent  
>>>>>>> origin/master

1. Weighting
2. Simple use of self-reported turnout? (Gallup paper: distinguish deterministic and probabilistic methods)

<!-- too detailed for the introduction 
The general solution to non-representativeness is weighting. With online surveys you need to do a little more, instead of doing a stratification-design to draw from certain strata you need to define the strata afterwards and weight your responses accordingly (post-stratification). This can be done with Census data that gives you the frequencies of the strata in the population. Then, when you made your survey representative of the population, in a second step you do what all polls have to do. Account for self-selection and non-response bias and actual-voter bias: -> Therefore, weight according to likelihood of actual voting and party affiliation of certain strata. (Here I want to point out the two steps!)

Now: Our actual project. In our forecasting project we want to use non-representative polling data from mobile-phone app users. (Obtained from Dalia research, citation blabla) So we have a raw survey and we want to weight it. We want to dicuss severall weighting methods. And we want to make a forecast of the Bundestagswahl. blalablablubb -->

<!-- from Lumpley slides
Nonresponse introduces statistical inefficiency
•Nonresponse will also lead to biased estimates if the propensity to respond to the survey(or an item) is related to the attribute of interest
•Ex ante attempts to reduce unit and item nonreponse seldom are fully successful (e.g. increase and dispersion of contact attempts, incentives, conversion by specially trained interviewers, reduction of respondent burden/ interview length via matrix sampling etc.)
•Non response rate and non response bias seem hardly related
•Importance ofex post strategies for missing data
•Different strategies are based on different assumptions about missingness and different kinds of information available on those missing

Poststratification
•Ex poststratification of the sample along one or several categorical variables x…
-… which are suspected to co-determine missingness and the attribute of interest y („commoncauses“ according to Groves, 2006)
-… whose (joint) population distribution is known
•Conceive of the categories/ cross-classifications of x as groups or strata h, use the stratified estimator
•Corrects for disproportionalities between NhN and nh′N that may be either due to sampling or due to non response
•Assumes MAR

Raking
-> if only marginal distributions are known (as for example with the forschungsgruppe wahlen)
--> 

## Approach number one

For stratifying the data we orient ourself at the work of XXX. The basic idea is to compute clusters of voters along several demographic categories and use their past votes to compute weights.

First, if possible we use post-stratification (see Lumpley, ch. 7?) to compute weights for subgroups in the sample. Post-stratification tries to make a sample representative of the actual population by ensuring the relative size of subgroup resembles the relative size of the same subgroup in the 'true' population. For forecasting the 'true' population is not known, as it is a question of who will actually turn out to vote. 

How we plan to make poll representative

Compare different stratification approaches

Likely problems we will encounter: 1. empthy clusters or clusters with low number of observations. Implications: If empthy, there is a real problem. If the number of observation is low, e.g. below 20, the weigths will will amplify the impact of this small group in the total forecasting result.

Benchmark -> other publically available polls. This is straight-forward, but also problematic as it might induce a herding effect. The final evaluation is only possibe after the election

## Approach number two


# Data Overview

how representative our data already is

1. raw data forecast. Compared to other forecastes the data under represents the CDU as well as the SPD. (Verify) 

2. Show distribution of respondents on different demographic clusters and compare to zensus / exit polls / election statistics

3. Raw (voted last election)

# Results

What the result is of making it representative

1. Election forecast Weighted with exit polls

2. Election forecast weighted with election statics 

3. Election forecast weighted with zensus

Compare the three different weighten approaches

Comment Moritz: I think we have to weight the data with Zensus data in any case at least for gender and age as long we don't want to use the Dalia weights; then we can either use election statistics or exit poll data (that we don't have)

My approach would be:

1. Weighting with Zensus (accounts for self-selection of the survey)
2. Different mixes of weighting with election statistics with education
(accounts for likely voter bias)


# Conclusion

Summary of the core finding

Further implications

# References

<!-- to be copied in the .bib file
% This file was created with Citavi 5.5.0.1

@misc{PewResearchCenter.2012,
 date = {2012},
 title = {Assessing the Representativeness of Public Opinion Surveys},
 url = {http://www.people-press.org/files/legacy-pdf/Assessing%20the%20Representativeness%20of%20Public%20Opinion%20Surveys.pdf},
 institution = {{Pew Research Center}}
}





@article{Skibba.2016,
 author = {Skibba, Ramin},
 year = {2016},
 title = {The polling crisis: How to tell what people really think},
 keywords = {Bias (Epidemiology);Canada;Cell Phones/economics/utilization;Continental Population Groups/statistics {\&} numerical data;Data Collection/methods/standards;Datasets as Topic;Demography/statistics {\&} numerical data;Educational Status;Female;Humans;Internet/economics/utilization;Male;Politics;Public Opinion;Surveys and Questionnaires/economics/standards;Uncertainty;United Kingdom;United States},
 urldate = {01.04.2017},
 pages = {304--306},
 volume = {538},
 number = {7625},
 journal = {Nature},
 doi = {10.1038/538304a}
}

@article{Wang.2015,
 abstract = {International Journal of Forecasting, Corrected proof. doi:10.1016/j.ijforecast.2014.06.001},
 author = {Wang, Wei and Rothschild, David and Goel, Sharad and Gelman, Andrew},
 year = {2015},
 title = {Forecasting elections with non-representative polls},
 keywords = {Election forecasting;Multilevel regression and poststratification;Non-representative polling},
 urldate = {01.04.2017},
 pages = {980--991},
 volume = {31},
 number = {3},
 issn = {01692070},
 journal = {International Journal of Forecasting},
 doi = {10.1016/j.ijforecast.2014.06.001}
}

@article{Mellon.2015,
 author = {Mellon, Jonathan and Prosser, Chris},
 year = {2015},
 title = {Investigating the Great British Polling Miss: Evidence from the British Election Study},
 issn = {1556-5068},
 journal = {SSRN Electronic Journal},
 doi = {10.2139/ssrn.2631165}
}

@article{Keeter.2006,
 abstract = {Declining contact and cooperation rates in random digit dial (RDD) national telephone surveys raise serious concerns about the validity of estimates drawn from such research. While research in the 1990s indicated that nonresponse bias was relatively small, response rates have continued to fall since then. The current study replicates a 1997 methodological experiment that compared results from a {\dq}Standard{\dq} 5-day survey employing the Pew Research Center's usual methodology with results from a {\dq}Rigorous{\dq} survey conducted over a much longer field period and achieving a significantly higher response rate. As with the 1997 study, there is little to suggest that unit nonresponse within the range of response rates obtained seriously threatens the quality of survey estimates. In 77 out of 84 comparable items, the two surveys yielded results that were statistically indistinguishable. While the {\dq}Rigorous{\dq} study respondents tended to be somewhat less politically engaged, they did not report consistently different behaviors or attitudes on other kinds of questions. With respect to sample composition, the Standard survey was closely aligned with estimates from the U.S. Census and other large government surveys on most variables. We extend our analysis of nonresponse to include comparisons with the hardest-to-reach respondents and with respondents who terminated the interview prior to completion.},
 author = {Keeter, Scott and Kennedy, Courtney and Dimock, Michael and Best, Jonathan and Craighill, Peyton},
 year = {2006},
 title = {Gauging the Impact of Growing Nonresponse on Estimates from a National RDD Telephone Survey},
 url = {http://www.jstor.org/stable/4124225},
 pages = {759--779},
 volume = {70},
 number = {5},
 issn = {15375331},
 journal = {The Public Opinion Quarterly}
}

% This file was created with Citavi 5.5.0.1

@article{Yeager.2011,
 author = {Yeager, D. S. and Krosnick, J. A. and Chang, L. and Javitz, H. S. and Levendusky, M. S. and Simpser, A. and Wang, R.},
 year = {2011},
 title = {Comparing the Accuracy of RDD Telephone Surveys and Internet Surveys Conducted with Probability and Non-Probability Samples},
 pages = {709--747},
 volume = {75},
 number = {4},
 journal = {Public Opinion Quarterly},
 doi = {10.1093/poq/nfr020}
}

@incollection{Holbrook.2007,
 author = {Holbrook, Allyson L. and Krosnick, Jon A. and Pfent, Alison},
 title = {The Causes and Consequences of Response Rates in Surveys by the News Media and Government Contractor Survey Research Firms},
 pages = {499--528},
 publisher = {{John Wiley {\&} Sons, Inc}},
 isbn = {9780470173404},
 editor = {Lepkowski, James M. and Tucker, Clyde and Brick, J. Michael and Leeuw, Edith D. de and Japec, Lilli and Lavrakas, Paul J. and Link, Michael W. and Sangster, Roberta L.},
 booktitle = {Advances in Telephone Survey Methodology},
 year = {2007},
 address = {Hoboken, NJ, USA},
 doi = {10.1002/9780470173404.ch23}
}


@article{Squire.1988,
 author = {Squire, Peverill},
 year = {1988},
 title = {Why the 1936 Literary Digest Poll Failed},
 url = {http://www.jstor.org/stable/2749114},
 urldate = {01.04.2017},
 pages = {125--133},
 volume = {52},
 number = {1},
 issn = {15375331},
 journal = {The Public Opinion Quarterly}
}

@book{Groves.2009,
author = "Groves, Robert M. and Floyd J. Fowler Jr. and Mick P. Couper and James M.
Lepkowski and Eleanor Singer and Roger Tourangeau",
title = "Survey Methodology",
publisher  = "John Wiley & Sons Inc",
year = "2009",
address = "New Jersey",
edition = 2
}



-->






