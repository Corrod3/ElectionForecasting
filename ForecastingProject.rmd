---
title: |
  | Non-Representative Polling
  | Mobile User Polling Data for the German Federal Election
subtitle: "Election Forecasting Project"
author: "Moritz Hemmerlein & Alexander Sacharow"
abstract: "Using online surveys to forecast election outcomes imposes severe challenges to pollsters. Non-representativ samples and likely-voter bias skew gathered information and require adequate weighting. This paper sets out to discuss approaches of weighting-design to adjust a non-representative online poll at hand on the upcoming german federal election."
header-includes:
    - \usepackage{fancyhdr}
    - \pagestyle{fancy}
    - \fancyhead[LO,LE]{Non-represenative polling}
    - \fancyfoot[LO,LE]{Election Forecasting Project}
    - \fancyfoot[RE,RO]{Moritz Hemmerlein and Alexander Sacharow}
    - \usepackage{setspace}
    - \onehalfspacing
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    toc: true
    number_sections: true
fontfamily: mathpazo
fontsize: 11pt
urlcolor: blue
bibliography:
    -  literature.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Clear Global environment
rm(list=ls())

# Setting Working directory
try(setwd("D:/Eigene Datein/Dokumente/Uni/Hertie/Materials/Election Forecasting/ElectionForecasting"), silent = TRUE)
try(setwd("C:\\Users\\Moritz\\Desktop\\ElectionForecasting"), silent = TRUE)

source("main.R")

# Collect packages/libraries we need for paper:
packages <- c("fontcm", "psych", "plotly")

# install packages if not installed before
for (p in packages) {
  if (p %in% installed.packages()[,1]) {
    require(p, character.only=T)
  }
  else {
    install.packages(p, repos="http://cran.rstudio.com", dependencies = TRUE)
    require(p, character.only=T)
  }
}
rm(p, packages)
```

# Introduction

<!--The problem -->
The digitialization is challenging the way polling was done for many decades. Calling people on their landline phones has become difficult as response rates drop and households equiped with such phones are getting less and less [@Skibba.2016]. In response, polling institutions have resorted to other methods for polling ranging from face-to-face interviews to mobile phone calling. However, these methods either face similar difficulties to ensure representativeness or are too expensive for regular polling. In order to tackle the obstacles, pollsters have began to experiment with non-representative polling methods [@Wang.2015].

<!-- The problem[cont.]
A: above we talk about non-representative polling, now it was online polling. I streamlined the narrative. -->
The major aim of pollsters in using non-representative surveys is not genuinely different from traditional goals of polls. The task is to gather responses which are representative for the population with regards to demographic and other criteria. The major difference is that representative cannot be ensured before the actual poll takes place, but is introduced ex-post. For example, in the case of online surveys, respondents are more likely to be from certain age groups or with a particular political background depending on the website where the survey is conducted. The non-representative poll can, however, be post-stratified in order to reflect the replies from the true population.

<!-- A: maybe we should also address the likely voter problem / turnout here so we can justify to discuss two problems in the design paper, in the end of the discussion we can just say that for practical reasons we just used the self-reported intended vote participation for pracitcal reasons.

What make election forecasting distinct from other polling efforts is the problem of turnout. Hence, the 'true' population is not the actual electorate but the actual voters. Often elections are not necessarily decided by changes in voter's intention, but by who is going to the polls. For election forecasting we therefore need a coherent idea of likely voters.
-->

<!-- aim / question-->
In our paper we want to explore and analyse different approaches to post-stratification for non-representative election polls of the German federal election 2017. We work with individual level data from mobile-phone app users. This raw data will then be post-stratified with different statistics about voters in Germany. Ultimately, we want to identify the most promising approach in order to generate an election forecast. As the ultimate election will only be after the end of the research, we can compare our post-stratified election forecast only with other forecasts of the 2017 federal election.

<!-- LATER: here results of our paper -->

<!-- Structure  -->
The paper is structured as follows: Chapter two will survey the literature on online polls, their representativeness and approaches to employ such data to forecast elections. Proceedingly, we present our data and discuss several issues of representativeness and measurement error that need to be accounted for. In chapter four we present two (or three) methods want to apply to weight our data at hand. Chapter five is presenting and dicussing our preliminary results. Chapter six concludes and gives an outlook on practical obstacles and methodological issues that our approach suffers from.

# Related Literature

<!-- problems of traditional polling -->
Traditional polling and in particular election polling has relied heavily on telephone surveys for the last decades. To ensure representativeness the standard was randomized digit dialing (RDD). The selection of random respondents was intended to eliminate the sample bias of the survey. However, for several reasons this approach has become unreliables. First, response rates have declined heavily [@Keeter.2006; @Holbrook.2007]. XXX reported that the response rate is down to X% in XXXX, compared to X% in XXXX or even X% in XXXX. <!-- add reasons here --> This induces a non-response bias to polls which cannot be handled with classical approaches anymore. Second, more and more people don't get landlines telephones after moving to new places or just give them up as mobile phone and other means of communication have increasingly become popular.<!-- source --> This induces a sample bias which the RDD approach intended to eliminate. How problematic this can be was famously illustrated by the Literature Digest poll in 1936, which failed to realized its biased sample [@Squire.1988]. But the problem still exists, as non-representatives was for example a major problem when forecast the UK Gereral elections in 2015 [@Mellon.2015].

<!-- Papers on non-representative polling -->
Can non-representative polls fix this? A study by @Yeager.2011 is rather pessimistic. They have compared telephone and online polls and argued that despite the problems of phone surveys they are still more accurate then online polls. However, they used only simple post-stratification methods. <!-- a few more words on the study here-->

@Wang.2015 in contrast are much more optimist about the possiblities of non-representative polling. They used polling results from XBox users which were highly unrepresentative of the population to forecast the 2012 U.S. presidential elections. By employing a sophisticated multi-stage approach to stratify and calibrate the data they were able to generate accurate forecasts of the elections. 

The difference between the two studies illustrates that the methods for stratifying and calibrating the forecasts are the key to meaningful forecasts based on non-representative polling. 

<!-- Methods on correcting polls
A: or we move this to the methological section -->
1. post stratification 
2. raking
3. multiple level regression + post stratification
4. random sampling?

<!-- papers on likely voter estimation (if we include it) -->

Typical biases in polling (most important: non-response bias; sample-selection bias)
    Which biases do we expect in the data
    
   

# Data and Potential Biases

<!-- raw data for vote intentions -->
The forecasting project utilizes data from the Europulse Survey which is conducted quarterly by [Dalia Research](https://daliaresearch.com/) in all EU countries. The survey consists of seven waves, but for this project we only use two waves of the survey from December 2016 and March 2017. The first wave is freely available on [Kaggle](https://www.kaggle.com/daliaresearch/trump-effect), the second wave was provided to us directly by Dalia Research. Each wave consists of about 11000 individuals, of which roughly 1900 were from Germany. The data is already pre-stratified by Dalia Research based on micro zensus data for age and gender. 

<!-- data for post-stratification -->
In order to post-stratify the data, we want to use different sources and compare their effect on the forecast: (1) exit poll data from Forschungsgruppe Wahlen or intratest dimap Institute (what), (2) representative election statistics and (3) microzensus data. 

<!-- data for likely-voter debate -->

General: include how did we get the data...

(dicuss biases along the framework of potential sources of bias measurement and representativeness)



# Weighting Approach
<!-- too detailed for the introduction -->
The general solution to non-representativeness is weighting. With online surveys you need to do a little more, instead of doing a stratification-design to draw from certain strata you need to define the strata afterwards and weight your responses accordingly (post-stratification). This can be done with Zensus data that gives you the frequencies of the strata in the population. Then, when you made your survey representative of the population, in a second step you do what all polls have to do. Account for self-selection and non-response bias and actual-voter bias: -> Therefore, weight according to likelihood of actual voting and party affiliation of certain strata. (Here I want to point out the two steps!)

Now: Our actual project. In our forecasting project we want to use non-representative polling data from mobile-phone app users. (Obtained from Dalia research, citation blabla) So we have a raw survey and we want to weight it. We want to dicuss severall weighting methods. And we want to make a forecast of the Bundestagswahl. blalablablubb

<!-- from Lumpley slides
Nonresponse introduces statistical inefficiency
•Nonresponse will also lead to biased estimates if the propensity to respond to the survey(or an item) is related to the attribute of interest
•Ex ante attempts to reduce unit and item nonreponse seldom are fully successful (e.g. increase and dispersion of contact attempts, incentives, conversion by specially trained interviewers, reduction of respondent burden/ interview length via matrix sampling etc.)
•Non response rate and non response bias seem hardly related
•Importance ofex post strategies for missing data
•Different strategies are based on different assumptions about missingness and different kinds of information available on those missing

Poststratification
•Ex poststratification of the sample along one or several categorical variables x…
-… which are suspected to co-determine missingness and the attribute of interest y („commoncauses“ according to Groves, 2006)
-… whose (joint) population distribution is known
•Conceive of the categories/ cross-classifications of x as groups or strata h, use the stratified estimator
•Corrects for disproportionalities between NhN and nh′N that may be either due to sampling or due to non response
•Assumes MAR

Raking
-> if only marginal distributions are known (as for example with the forschungsgruppe wahlen)
--> 

## Approach number one

For stratifying the data we orient ourself at the work of XXX. The basic idea is to compute clusters of voters along several demographic categories and use their past votes to compute weights.

First, if possible we use post-stratification (see Lumpley, ch. 7?) to compute weights for subgroups in the sample. Post-stratification tries to make a sample representative of the actual population by ensuring the relative size of subgroup resembles the relative size of the same subgroup in the 'true' population. For forecasting the 'true' population is not known, as it is a question of who will actually turn out to vote. 

How we plan to make poll representative

Compare different stratification approaches

Likely problems we will encounter: 1. empthy clusters or clusters with low number of observations. Implications: If empthy, there is a real problem. If the number of observation is low, e.g. below 20, the weigths will will amplify the impact of this small group in the total forecasting result.

Benchmark -> other publically available polls. This is straight-forward, but also problematic as it might induce a herding effect. The final evaluation is only possibe after the election

## Approach number two


# Data Overview

how representative our data already is

1. raw data forecast. Compared to other forecastes the data under represents the CDU as well as the SPD. (Verify) 

2. Show distribution of respondents on different demographic clusters and compare to zensus / exit polls / election statistics

3. Raw (voted last election)

# Results

What the result is of making it representative

1. Election forecast Weighted with exit polls

2. Election forecast weighted with election statics 

3. Election forecast weighted with zensus

Compare the three different weighten approaches

# Conclusion

Summary of the core finding

Further implications

# References

<!-- to be copied in the .bib file

@article{Skibba.2016,
 author = {Skibba, Ramin},
 year = {2016},
 title = {The polling crisis: How to tell what people really think},
 keywords = {Bias (Epidemiology);Canada;Cell Phones/economics/utilization;Continental Population Groups/statistics {\&} numerical data;Data Collection/methods/standards;Datasets as Topic;Demography/statistics {\&} numerical data;Educational Status;Female;Humans;Internet/economics/utilization;Male;Politics;Public Opinion;Surveys and Questionnaires/economics/standards;Uncertainty;United Kingdom;United States},
 urldate = {01.04.2017},
 pages = {304--306},
 volume = {538},
 number = {7625},
 journal = {Nature},
 doi = {10.1038/538304a}
}

@article{Wang.2015,
 abstract = {International Journal of Forecasting, Corrected proof. doi:10.1016/j.ijforecast.2014.06.001},
 author = {Wang, Wei and Rothschild, David and Goel, Sharad and Gelman, Andrew},
 year = {2015},
 title = {Forecasting elections with non-representative polls},
 keywords = {Election forecasting;Multilevel regression and poststratification;Non-representative polling},
 urldate = {01.04.2017},
 pages = {980--991},
 volume = {31},
 number = {3},
 issn = {01692070},
 journal = {International Journal of Forecasting},
 doi = {10.1016/j.ijforecast.2014.06.001}
}

@article{Mellon.2015,
 author = {Mellon, Jonathan and Prosser, Chris},
 year = {2015},
 title = {Investigating the Great British Polling Miss: Evidence from the British Election Study},
 issn = {1556-5068},
 journal = {SSRN Electronic Journal},
 doi = {10.2139/ssrn.2631165}
}

@article{Keeter.2006,
 abstract = {Declining contact and cooperation rates in random digit dial (RDD) national telephone surveys raise serious concerns about the validity of estimates drawn from such research. While research in the 1990s indicated that nonresponse bias was relatively small, response rates have continued to fall since then. The current study replicates a 1997 methodological experiment that compared results from a {\dq}Standard{\dq} 5-day survey employing the Pew Research Center's usual methodology with results from a {\dq}Rigorous{\dq} survey conducted over a much longer field period and achieving a significantly higher response rate. As with the 1997 study, there is little to suggest that unit nonresponse within the range of response rates obtained seriously threatens the quality of survey estimates. In 77 out of 84 comparable items, the two surveys yielded results that were statistically indistinguishable. While the {\dq}Rigorous{\dq} study respondents tended to be somewhat less politically engaged, they did not report consistently different behaviors or attitudes on other kinds of questions. With respect to sample composition, the Standard survey was closely aligned with estimates from the U.S. Census and other large government surveys on most variables. We extend our analysis of nonresponse to include comparisons with the hardest-to-reach respondents and with respondents who terminated the interview prior to completion.},
 author = {Keeter, Scott and Kennedy, Courtney and Dimock, Michael and Best, Jonathan and Craighill, Peyton},
 year = {2006},
 title = {Gauging the Impact of Growing Nonresponse on Estimates from a National RDD Telephone Survey},
 url = {http://www.jstor.org/stable/4124225},
 pages = {759--779},
 volume = {70},
 number = {5},
 issn = {15375331},
 journal = {The Public Opinion Quarterly}
}

% This file was created with Citavi 5.5.0.1

@article{Yeager.2011,
 author = {Yeager, D. S. and Krosnick, J. A. and Chang, L. and Javitz, H. S. and Levendusky, M. S. and Simpser, A. and Wang, R.},
 year = {2011},
 title = {Comparing the Accuracy of RDD Telephone Surveys and Internet Surveys Conducted with Probability and Non-Probability Samples},
 pages = {709--747},
 volume = {75},
 number = {4},
 journal = {Public Opinion Quarterly},
 doi = {10.1093/poq/nfr020}
}

@incollection{Holbrook.2007,
 author = {Holbrook, Allyson L. and Krosnick, Jon A. and Pfent, Alison},
 title = {The Causes and Consequences of Response Rates in Surveys by the News Media and Government Contractor Survey Research Firms},
 pages = {499--528},
 publisher = {{John Wiley {\&} Sons, Inc}},
 isbn = {9780470173404},
 editor = {Lepkowski, James M. and Tucker, Clyde and Brick, J. Michael and Leeuw, Edith D. de and Japec, Lilli and Lavrakas, Paul J. and Link, Michael W. and Sangster, Roberta L.},
 booktitle = {Advances in Telephone Survey Methodology},
 year = {2007},
 address = {Hoboken, NJ, USA},
 doi = {10.1002/9780470173404.ch23}
}


@article{Squire.1988,
 author = {Squire, Peverill},
 year = {1988},
 title = {Why the 1936 Literary Digest Poll Failed},
 url = {http://www.jstor.org/stable/2749114},
 urldate = {01.04.2017},
 pages = {125--133},
 volume = {52},
 number = {1},
 issn = {15375331},
 journal = {The Public Opinion Quarterly}
}

-->






